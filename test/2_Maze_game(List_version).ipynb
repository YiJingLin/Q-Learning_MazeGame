{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    def __init__(self, epsilon, gamma, alpha, \n",
    "                 state, actions, q_table):\n",
    "        self.epsilon = epsilon # epsilon-greedy policy, default = 0.9\n",
    "        self.gamma = gamma # decay-rate, default = 0.9\n",
    "        self.alpha = alpha # learning-rate, default = 0.1\n",
    "        \n",
    "        self.state = state # coordinate tuple, example, (2,3)\n",
    "        self.actions = actions # 1-D action list, ['up', 'down', 'left', 'right']\n",
    "        self.q_table = q_table # 3-D array, 3rd dimension for action reward\n",
    "    \n",
    "    def choose_action(self):\n",
    "        state_actions = self.q_table[self.state] # numpy \n",
    "        # get random rate and compare to epsilon\n",
    "        randomRate = np.random.uniform()\n",
    "        if randomRate > self.epsilon :\n",
    "            action = self.actions.index(np.random.choice(self.actions)) # choose randomly\n",
    "        elif state_actions.max() == 0 :\n",
    "            p=[]\n",
    "            count=0\n",
    "            for idx, reward in enumerate(state_actions) :\n",
    "                if reward == 0:\n",
    "                    p.append(1)\n",
    "                    count+=1\n",
    "                else:\n",
    "                    p.append(0)\n",
    "            \n",
    "            p = np.array(p)/count\n",
    "            action = self.actions.index(np.random.choice(self.actions, p=list(p)))\n",
    "        else:\n",
    "            action = state_actions.argmax() # derive action's index with highest reward\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update_q_table(self, reward, action, nxt_state):\n",
    "        # Q predict : choose action's reward at current state on q_table\n",
    "        q_predict = self.q_table[self.state][action]\n",
    "        # Q target : consider next step reward\n",
    "        if nxt_state in ['win', 'fail'] :\n",
    "            q_target = reward\n",
    "        else:\n",
    "            q_target = reward + self.gamma * self.q_table[nxt_state][action]\n",
    "        # accumulate\n",
    "        self.q_table[self.state][action] += self.alpha * (q_target - q_predict) \n",
    "        self.state = nxt_state # update state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class maze_env:\n",
    "    def __init__(self, actions):\n",
    "        self.unit = 40\n",
    "        self.actions = actions\n",
    "        \n",
    "    def build_map(self, size, target, t_reward, fail_list, f_reward_list):\n",
    "        # check target\n",
    "        if not (target[0] < size[0] and target[1] < size[1]) :\n",
    "            print(\"target out of bound !\")\n",
    "            return\n",
    "        if not t_reward > 0:\n",
    "            print(\"target\\'s reward should be positive !\")\n",
    "        # check fail block\n",
    "        for idx, fail in enumerate(fail_list):\n",
    "            if not (fail[0] < size[0] and fail[1] < size[1]):\n",
    "                print('No '+str(idx)+'. fail block out of bound !')\n",
    "                return\n",
    "            if not f_reward_list[idx] < 0:\n",
    "                print('No '+str(idx)+'. fail\\'s reward should be negative !')\n",
    "                return \n",
    "            if fail == target :\n",
    "                print('No '+str(idx)+'. fail have same coordinate with target !')\n",
    "                return\n",
    "        self.size = size\n",
    "        self.target = target\n",
    "        self.fail_list = fail_list\n",
    "        \n",
    "        self.map = np.zeros((size))\n",
    "        self.map[target] = t_reward\n",
    "        for idx, fail in enumerate(fail_list):\n",
    "            self.map[fail] = f_reward_list[idx]\n",
    "        \n",
    "        print('game map : ')\n",
    "        print(self.map)\n",
    "        return\n",
    "    \n",
    "    def env_feedback(self, state, action):\n",
    "        # strategy : calculate coordinate first, and then check finish or not\n",
    "        nxt_state = self.cal_coordinate(state, action)\n",
    "        \n",
    "        reward = self.map[nxt_state]\n",
    "        if nxt_state in self.fail_list:\n",
    "            nxt_state = 'fail'\n",
    "        elif nxt_state == self.target:\n",
    "            nxt_state = 'win'\n",
    "            \n",
    "        return nxt_state, reward\n",
    "    \n",
    "    def cal_coordinate(self, state, action):\n",
    "        nxt_state = ()\n",
    "        move = ()\n",
    "        \n",
    "        if action == 0: # up\n",
    "            move = (-1,0)\n",
    "            if state[0] == 0: \n",
    "                nxt_state = state # hit the top wall\n",
    "            else: \n",
    "                nxt_state = tuple([sum(x) for x in zip(state,move)])\n",
    "                \n",
    "        elif action == 1: #down\n",
    "            move = (1,0)\n",
    "            if state[0] == self.size[0]-1 : \n",
    "                nxt_state = state # hit the bottom wall\n",
    "            else: \n",
    "                nxt_state = tuple([sum(x) for x in zip(state,move)])\n",
    "                \n",
    "        elif action == 2: #left\n",
    "            move = (0,-1)\n",
    "            if state[1] == 0: \n",
    "                nxt_state = state # hit the left wall\n",
    "            else: \n",
    "                nxt_state = tuple([sum(x) for x in zip(state,move)])\n",
    "                \n",
    "        elif action == 3: # right\n",
    "            move = (0,1)\n",
    "            if state[1] == self.size[1]-1 : \n",
    "                nxt_state = state # hit the right wall\n",
    "            else: \n",
    "                nxt_state = tuple([sum(x) for x in zip(state,move)])\n",
    "        \n",
    "        return nxt_state\n",
    "    \n",
    "    def create_q_table(self):\n",
    "        q_table = np.zeros(self.size + (len(self.actions),))\n",
    "        print('Q_table.shape :')\n",
    "        print(q_table.shape)\n",
    "        return np.array(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 20\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "initSTATE = (0,0)\n",
    "SIZE = (3,3) # maze size\n",
    "\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game map : \n",
      "[[ 0.  0. -2.]\n",
      " [ 0.  0. -2.]\n",
      " [ 0.  0.  5.]]\n"
     ]
    }
   ],
   "source": [
    "target = (2,2)\n",
    "t_reward = 5\n",
    "fail_list = [(0,2),(1,2)]\n",
    "f_reward_list = np.random.randint(low=-7, high=-1, size=len(fail_list))\n",
    "\n",
    "Maze = maze_env(ACTIONS)\n",
    "Maze.build_map(SIZE, target, t_reward, fail_list, f_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_table.shape :\n",
      "(3, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "Q_table = Maze.create_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent = agent(epsilon= EPSILON, gamma= GAMMA, alpha= ALPHA,\n",
    "             state = initSTATE, actions= ACTIONS,q_table= Q_table.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0) > (0, 0) > (0, 0) > (0, 0) > (1, 0) > (1, 1) > fail\n",
      " Episode. 0 finished ... ,total step : 7\n",
      "(0, 0) > (1, 0) > (0, 0) > (0, 0) > (1, 0) > (0, 0) > (0, 0) > (0, 1) > (0, 1) > fail\n",
      " Episode. 1 finished ... ,total step : 10\n",
      "(0, 0) > (0, 1) > (1, 1) > (0, 1) > (0, 1) > (0, 1) > (0, 1) > (0, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 0) > (2, 0) > (1, 0) > (2, 0) > (2, 1) > (2, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 0) > (2, 0) > (2, 1) > (1, 1) > (0, 1) > (1, 1) > (2, 1) > (2, 0) > (2, 0) > (2, 0) > (2, 0) > (2, 1) > (1, 1) > (1, 0) > (2, 0) > (2, 0) > (2, 0) > (2, 0) > (2, 1) > (1, 1) > (0, 1) > (0, 1) > (0, 1) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 1) > (1, 1) > (2, 1) > win\n",
      " Episode. 2 finished ... ,total step : 56\n",
      "(0, 0) > (1, 0) > (1, 0) > (1, 1) > (2, 1) > win\n",
      " Episode. 3 finished ... ,total step : 6\n",
      "(0, 0) > (0, 0) > (0, 0) > (1, 0) > (2, 0) > (1, 0) > (2, 0) > (2, 0) > (2, 1) > (2, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 4 finished ... ,total step : 15\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 5 finished ... ,total step : 4\n",
      "(1, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 6 finished ... ,total step : 10\n",
      "(0, 0) > (0, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 7 finished ... ,total step : 7\n",
      "(0, 0) > (0, 0) > (1, 0) > (2, 0) > (2, 1) > (1, 1) > fail\n",
      " Episode. 8 finished ... ,total step : 7\n",
      "(0, 0) > (0, 0) > (1, 0) > (1, 0) > (0, 0) > (0, 0) > (0, 0) > (1, 0) > (1, 0) > (0, 0) > (1, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 9 finished ... ,total step : 16\n",
      "(1, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 10 finished ... ,total step : 5\n",
      "(0, 0) > (1, 0) > (0, 0) > (0, 0) > (0, 0) > (1, 0) > (0, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 1) > (1, 1) > (0, 1) > fail\n",
      " Episode. 11 finished ... ,total step : 14\n",
      "(0, 0) > (1, 0) > (1, 0) > (1, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 12 finished ... ,total step : 9\n",
      "(0, 0) > (0, 0) > (0, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 13 finished ... ,total step : 7\n",
      "(0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 14 finished ... ,total step : 10\n",
      "(0, 1) > (0, 1) > (0, 1) > (0, 1) > (1, 1) > (1, 0) > (1, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 15 finished ... ,total step : 16\n",
      "(0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (0, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 1) > (2, 0) > (2, 1) > (1, 1) > (0, 1) > (1, 1) > (1, 0) > (2, 0) > (2, 1) > (2, 0) > (2, 1) > win\n",
      " Episode. 16 finished ... ,total step : 24\n",
      "(0, 0) > (0, 0) > (0, 0) > (1, 0) > (0, 0) > (0, 0) > (0, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 17 finished ... ,total step : 11\n",
      "(1, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 18 finished ... ,total step : 5\n",
      "(0, 0) > (0, 0) > (1, 0) > (0, 0) > (0, 0) > (0, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 19 finished ... ,total step : 11\n"
     ]
    }
   ],
   "source": [
    "def path(state, is_terminated):\n",
    "    print(state, end='')\n",
    "    if not is_terminated: print(' > ', end='')\n",
    "\n",
    "# main process RL - Q_Learning\n",
    "for episode in range(EPISODES):\n",
    "    Agent.state = initSTATE\n",
    "    is_terminated = False\n",
    "    count = 0\n",
    "    while not is_terminated :\n",
    "        # choose action and get env. feedback\n",
    "        action = Agent.choose_action()\n",
    "#         print('action:'+str(action), end='')\n",
    "        nxt_state, reward = Maze.env_feedback(state=Agent.state, action=action)\n",
    "#         print('nxt_state:'+str(nxt_state), end='')\n",
    "        # update Q_table\n",
    "        Agent.update_q_table(reward=reward, action=action, nxt_state=nxt_state)\n",
    "        \n",
    "        if nxt_state in ['win','fail']:\n",
    "            is_terminated = True\n",
    "        # update visual info\n",
    "        path(Agent.state, is_terminated)\n",
    "        Agent.state = nxt_state\n",
    "        count +=1\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    print('\\n Episode. '+str(episode)+' finished ... ,total step : '+str(count))\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        , -0.06498   ],\n",
       "        [ 0.        ,  0.        ,  0.        , -0.38      ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , -0.018     ],\n",
       "        [ 0.        ,  0.        ,  0.        , -0.38      ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        ,  2.58100386],\n",
       "        [ 0.        ,  0.        ,  0.        ,  4.07348991],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent.q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give punishment when bumping the wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) > (0, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 0 finished ... ,total step : 6\n",
      "(1, 0) > (0, 0) > (1, 0) > (0, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 1 finished ... ,total step : 8\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 2 finished ... ,total step : 4\n",
      "(0, 0) > (1, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 3 finished ... ,total step : 6\n",
      "(1, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 4 finished ... ,total step : 5\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 5 finished ... ,total step : 4\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 6 finished ... ,total step : 4\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 7 finished ... ,total step : 4\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 8 finished ... ,total step : 4\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 9 finished ... ,total step : 4\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 10 finished ... ,total step : 4\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 11 finished ... ,total step : 4\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 12 finished ... ,total step : 4\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 13 finished ... ,total step : 4\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 14 finished ... ,total step : 4\n",
      "(0, 0) > (1, 0) > (2, 0) > (2, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 15 finished ... ,total step : 7\n",
      "(1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 16 finished ... ,total step : 4\n",
      "(1, 0) > (0, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 17 finished ... ,total step : 6\n",
      "(1, 0) > (1, 1) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 18 finished ... ,total step : 6\n",
      "(1, 0) > (0, 0) > (1, 0) > (2, 0) > (2, 1) > win\n",
      " Episode. 19 finished ... ,total step : 6\n"
     ]
    }
   ],
   "source": [
    "def path(state, is_terminated):\n",
    "    print(state, end='')\n",
    "    if not is_terminated: print(' > ', end='')\n",
    "\n",
    "# main process RL - Q_Learning\n",
    "for episode in range(EPISODES):\n",
    "    Agent.state = initSTATE\n",
    "    is_terminated = False\n",
    "    count = 0\n",
    "    while not is_terminated :\n",
    "        # choose action and get env. feedback\n",
    "        action = Agent.choose_action()\n",
    "#         print('action:'+str(action), end='')\n",
    "        nxt_state, reward = Maze.env_feedback(state=Agent.state, action=action)\n",
    "#         print('nxt_state:'+str(nxt_state), end='')\n",
    "        # update Q_table\n",
    "        ###########################################\n",
    "        if Agent.state == nxt_state: # bump the wall\n",
    "            reward = -1\n",
    "        ###########################################\n",
    "        Agent.update_q_table(reward=reward, action=action, nxt_state=nxt_state)\n",
    "        \n",
    "        if nxt_state in ['win','fail']:\n",
    "            is_terminated = True\n",
    "        # update visual info\n",
    "        path(Agent.state, is_terminated)\n",
    "        Agent.state = nxt_state\n",
    "        count +=1\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    print('\\n Episode. '+str(episode)+' finished ... ,total step : '+str(count))\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.29701   , -0.01301303, -0.1       , -0.06498   ],\n",
       "        [ 0.        ,  0.        ,  0.        , -0.38      ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.06463971, -0.06159249, -0.199     , -0.0504    ],\n",
       "        [ 0.        ,  0.        , -0.01791   , -0.38      ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        , -0.199     ,  0.        ,  4.04141087],\n",
       "        [ 0.        ,  0.        ,  0.        ,  4.887358  ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent.q_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
