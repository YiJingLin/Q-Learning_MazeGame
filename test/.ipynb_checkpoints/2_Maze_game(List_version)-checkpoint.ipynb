{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-f98dd6936e5c>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f98dd6936e5c>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class agent:\n",
    "    def __init__(self, epsilon, gamma, alpha, \n",
    "                 state, actions, q_table):\n",
    "        self.epsilon = epsilon # epsilon-greedy policy, default = 0.9\n",
    "        self.gamma = gamma # decay-rate, default = 0.9\n",
    "        self.alpha = alpha # learning-rate, default = 0.1\n",
    "        \n",
    "        self.state = state # coordinate tuple, example, (2,3)\n",
    "        self.actions = actions # 1-D action list, ['up', 'down', 'left', 'right']\n",
    "        self.q_table = q_table # 3-D array, 3rd dimension for action reward\n",
    "    \n",
    "    def choose_action(self):\n",
    "        state_actions = self.q_table[self.state] # numpy \n",
    "        # get random rate and compare to epsilon\n",
    "        randomRate = np.random.uniform()\n",
    "        if randomRate > self.epsilon :\n",
    "            action = self.actions.index(np.random.choice(self.actions)) # choose randomly\n",
    "        elif state_actions.max() == 0 :\n",
    "            p=[]\n",
    "            count=0\n",
    "            for idx, reward in enumerate(state_actions) :\n",
    "                if reward == 0:\n",
    "                    p.append(1)\n",
    "                    count+=1\n",
    "                else:\n",
    "                    p.append(0)\n",
    "            \n",
    "            p = np.array(p)/count\n",
    "            action = self.actions.index(np.random.choice(self.actions, p=list(p))\n",
    "        else:\n",
    "            action = state_actions.argmax() # derive action's index with highest reward\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update_q_table(self, reward, action, nxt_state):\n",
    "        # Q predict : choose action's reward at current state on q_table\n",
    "        q_predict = self.q_table[self.state][action]\n",
    "        # Q target : consider next step reward\n",
    "        if nxt_state in ['win', 'fail'] :\n",
    "            q_target = reward\n",
    "        else:\n",
    "            q_target = reward + self.gamma * self.q_table[nxt_state][action]\n",
    "        # accumulate\n",
    "        self.q_table[self.state][action] += self.alpha * (q_target - q_predict) \n",
    "        self.state = nxt_state # update state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class maze_env:\n",
    "    def __init__(self, actions):\n",
    "        self.unit = 40\n",
    "        self.actions = actions\n",
    "        \n",
    "    def build_map(self, size, target, t_reward, fail_list, f_reward_list):\n",
    "        # check target\n",
    "        if not (target[0] < size[0] and target[1] < size[1]) :\n",
    "            print(\"target out of bound !\")\n",
    "            return\n",
    "        if not t_reward > 0:\n",
    "            print(\"target\\'s reward should be positive !\")\n",
    "        # check fail block\n",
    "        for idx, fail in enumerate(fail_list):\n",
    "            if not (fail[0] < size[0] and fail[1] < size[1]):\n",
    "                print('No '+str(idx)+'. fail block out of bound !')\n",
    "                return\n",
    "            if not f_reward_list[idx] < 0:\n",
    "                print('No '+str(idx)+'. fail\\'s reward should be negative !')\n",
    "                return \n",
    "            if fail == target :\n",
    "                print('No '+str(idx)+'. fail have same coordinate with target !')\n",
    "                return\n",
    "        self.size = size\n",
    "        self.target = target\n",
    "        self.fail_list = fail_list\n",
    "        \n",
    "        self.map = np.zeros((size))\n",
    "        self.map[target] = t_reward\n",
    "        for idx, fail in enumerate(fail_list):\n",
    "            self.map[fail] = f_reward_list[idx]\n",
    "        \n",
    "        print('game map : ')\n",
    "        print(self.map)\n",
    "        return\n",
    "    \n",
    "    def env_feedback(self, state, action):\n",
    "        # strategy : calculate coordinate first, and then check finish or not\n",
    "        nxt_state = self.cal_coordinate(state, action)\n",
    "        \n",
    "        reward = self.map[nxt_state]\n",
    "        if nxt_state in self.fail_list:\n",
    "            nxt_state = 'fail'\n",
    "        elif nxt_state == self.target:\n",
    "            nxt_state = 'win'\n",
    "            \n",
    "        return nxt_state, reward\n",
    "    \n",
    "    def cal_coordinate(self, state, action):\n",
    "        nxt_state = ()\n",
    "        move = ()\n",
    "        \n",
    "        if action == 0: # up\n",
    "            move = (-1,0)\n",
    "            if state[0] == 0: \n",
    "                nxt_state = state # hit the top wall\n",
    "            else: \n",
    "                nxt_state = tuple([sum(x) for x in zip(state,move)])\n",
    "                \n",
    "        elif action == 1: #down\n",
    "            move = (1,0)\n",
    "            if state[0] == self.size[0]-1 : \n",
    "                nxt_state = state # hit the bottom wall\n",
    "            else: \n",
    "                nxt_state = tuple([sum(x) for x in zip(state,move)])\n",
    "                \n",
    "        elif action == 2: #left\n",
    "            move = (0,-1)\n",
    "            if state[1] == 0: \n",
    "                nxt_state = state # hit the left wall\n",
    "            else: \n",
    "                nxt_state = tuple([sum(x) for x in zip(state,move)])\n",
    "                \n",
    "        elif action == 3: # right\n",
    "            move = (0,1)\n",
    "            if state[1] == self.size[1]-1 : \n",
    "                nxt_state = state # hit the right wall\n",
    "            else: \n",
    "                nxt_state = tuple([sum(x) for x in zip(state,move)])\n",
    "        \n",
    "        return nxt_state\n",
    "    \n",
    "    def create_q_table(self):\n",
    "        q_table = np.zeros(self.size + (len(self.actions),))\n",
    "        print('Q_table.shape :')\n",
    "        print(q_table.shape)\n",
    "        return np.array(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 100\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "initSTATE = (0,0)\n",
    "SIZE = (3,3) # maze size\n",
    "\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = (2,2)\n",
    "t_reward = 5\n",
    "fail_list = [(0,2),(1,2)]\n",
    "f_reward_list = np.random.randint(low=-7, high=-1, size=len(fail_list))\n",
    "\n",
    "Maze = maze_env(ACTIONS)\n",
    "Maze.build_map(SIZE, target, t_reward, fail_list, f_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = Maze.create_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent = agent(epsilon= EPSILON, gamma= GAMMA, alpha= ALPHA,\n",
    "             state = initSTATE, actions= ACTIONS,q_table= Q_table.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path(state, is_terminated):\n",
    "    print(state, end='')\n",
    "    if not is_terminated: print(' > ', end='')\n",
    "\n",
    "# main process RL - Q_Learning\n",
    "for episode in range(EPISODES):\n",
    "    Agent.state = initSTATE\n",
    "    is_terminated = False\n",
    "    count = 0\n",
    "    while not is_terminated :\n",
    "        # choose action and get env. feedback\n",
    "        action = Agent.choose_action()\n",
    "#         print('action:'+str(action), end='')\n",
    "        nxt_state, reward = Maze.env_feedback(state=Agent.state, action=action)\n",
    "#         print('nxt_state:'+str(nxt_state), end='')\n",
    "        # update Q_table\n",
    "        Agent.update_q_table(reward=reward, action=action, nxt_state=nxt_state)\n",
    "        \n",
    "        if nxt_state in ['win','fail']:\n",
    "            is_terminated = True\n",
    "        # update visual info\n",
    "        path(Agent.state, is_terminated)\n",
    "        Agent.state = nxt_state\n",
    "        count +=1\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    print('\\n Episode. '+str(episode)+' finished ... ,total step : '+str(count))\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give punish when bumping the wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def path(state, is_terminated):\n",
    "    print(state, end='')\n",
    "    if not is_terminated: print(' > ', end='')\n",
    "\n",
    "# main process RL - Q_Learning\n",
    "for episode in range(EPISODES):\n",
    "    Agent.state = initSTATE\n",
    "    is_terminated = False\n",
    "    count = 0\n",
    "    while not is_terminated :\n",
    "        # choose action and get env. feedback\n",
    "        action = Agent.choose_action()\n",
    "#         print('action:'+str(action), end='')\n",
    "        nxt_state, reward = Maze.env_feedback(state=Agent.state, action=action)\n",
    "#         print('nxt_state:'+str(nxt_state), end='')\n",
    "        # update Q_table\n",
    "        ###########################################\n",
    "        if Agent.state == nxt_state: # bump the wall\n",
    "            reward = -1\n",
    "        ###########################################\n",
    "        Agent.update_q_table(reward=reward, action=action, nxt_state=nxt_state)\n",
    "        \n",
    "        if nxt_state in ['win','fail']:\n",
    "            is_terminated = True\n",
    "        # update visual info\n",
    "        path(Agent.state, is_terminated)\n",
    "        Agent.state = nxt_state\n",
    "        count +=1\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    print('\\n Episode. '+str(episode)+' finished ... ,total step : '+str(count))\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Agent.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maze = maze_env(size=(5,3), actions=['up', 'down', 'left', 'right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=5\n",
    "a = np.random.randint(low=0, high=7, size=20)\n",
    "print(a.min())\n",
    "a = tuple(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for idx in range(5):\n",
    "    a = np.random.randint(1,6,(5,4))\n",
    "    time.sleep(1)\n",
    "    print('\\r{}'.format(a),end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIT = 40   # pixels\n",
    "MAZE_H = 4  # grid height\n",
    "MAZE_W = 4  # grid width\n",
    "\n",
    "\n",
    "class Maze(tk.Tk, object):\n",
    "    def __init__(self):\n",
    "        super(Maze, self).__init__()\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.title('maze')\n",
    "        self.geometry('{0}x{1}'.format(MAZE_H * UNIT, MAZE_H * UNIT))\n",
    "#         self._build_maze()\n",
    "        \n",
    "maze = Maze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((1,2,4))\n",
    "b = (0,1)\n",
    "a[b][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,1,3,2])\n",
    "a.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=(1,2);b=(1,2)\n",
    "# c = tuple([sum(x) for x in zip(a,b)])\n",
    "# c\n",
    "if a==b:\n",
    "    print('T')\n",
    "else:\n",
    "    print('F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3,4]\n",
    "a/5\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
