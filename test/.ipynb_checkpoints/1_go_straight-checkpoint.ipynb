{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# go straight game\n",
    "- its a 1-D (one dimension) problem with target T and ur status O on the line\n",
    "- Morvan tutorial ([link](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-1-general-rl/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component for Q-table\n",
    "ACTIONS = ['left','right']\n",
    "nSTATE = 10 # status 0-9\n",
    "data = [[0]*len(ACTIONS)]*len_line # init. table data with 0\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left  right\n",
       "0     0      0\n",
       "1     0      0\n",
       "2     0      0\n",
       "3     0      0\n",
       "4     0      0\n",
       "5     0      0\n",
       "6     0      0\n",
       "7     0      0\n",
       "8     0      0\n",
       "9     0      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = pd.DataFrame(data=data, columns=ACTIONS)\n",
    "Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPSILON = 0.8  # 80% choose highest reward action and 20% choose randomly\n",
    "\n",
    "def choose_action(state):\n",
    "    state_actions = Q_table.iloc[state]\n",
    "    # 隨機值大於0.8(epsilon value) 或 第一次進入該state --> 隨機選取action\n",
    "    if (np.random.uniform() > EPSILON) or (state_actions.all()==0):\n",
    "        return np.random.choice(ACTIONS) # 隨機選取action\n",
    "    else:\n",
    "        return state_actions.argmax() #derive highest reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting feedback from env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With state and action you take, you can get env. feedback\n",
    "def env_feedback(state, action):\n",
    "    if action == 'right':\n",
    "        if state == nSTATE-2: #before terminate\n",
    "            next_state = 'terminate'\n",
    "            reward = 1\n",
    "        else:\n",
    "            next_state = state +1\n",
    "            reward = 0\n",
    "    else: # move left\n",
    "        reward = 0 #this action wont reach target, so reward always = 0 in the action\n",
    "        if state == 0:\n",
    "            next_state = state # reah the wall :3\n",
    "        else:\n",
    "            next_state = state - 1\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# env and agent state visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_TIME = 0.1\n",
    "\n",
    "def update_visual_env(state, episode, step_count):\n",
    "    env_list = ['-']*(nSTATE-1)+['T']  # '---------T' is our environment\n",
    "    if state == 'terminate':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_count) # string\n",
    "        print('\\r{}'.format(interaction), end='\\n') # wont change line if end='\\n'\n",
    "        time.sleep(2) # sleep 2 sec\n",
    "        print('\\r                      ', end='') # refresh line\n",
    "    else : \n",
    "        env_list[state] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(STEP_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Q-Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 13\n",
    "GAMMA = 0.9 # decay rate\n",
    "ALPHA = 0.1 # learning rate\n",
    "\n",
    "# No need to think but follow Q-Learning algorithm\n",
    "def Q_Learning():\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        step_count = 0\n",
    "        state = 0\n",
    "        is_terminated = False\n",
    "        update_visual_env(state, episode, step_count) # update env and show\n",
    "        while not is_terminated:\n",
    "            action = choose_action(state)\n",
    "            next_state, reward = env_feedback(state, action) #in this case: {terminal:1,otherwise:0}\n",
    "            #print('state : '+ str(state))\n",
    "            q_predict = Q_table.ix[state, action] #get Q_table value\n",
    "            if next_state == 'terminate':\n",
    "                q_target = reward        # reward=1 ,\n",
    "                is_terminated = True   # also means episode finish\n",
    "            else:\n",
    "                # pick max reward at next state's actions,\n",
    "                # multiply by decay rate GAMMA, \n",
    "                # and Add reward and previous result together \n",
    "                q_target = reward + GAMMA * Q_table.iloc[next_state, :].max()\n",
    "            \n",
    "            # update Q-table with q_target and predict one,\n",
    "            # which q_target was measured by next state\n",
    "            Q_table.ix[state, action] += ALPHA * (q_target - q_predict)\n",
    "            state = next_state # finally update state\n",
    "            \n",
    "            #Optional :P (just for visualization)\n",
    "            update_visual_env(state, episode, step_count+1) # update env. after state update\n",
    "            step_count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.687153e-08</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.250581e-08</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.411861e-07</td>\n",
       "      <td>0.000452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.936000e-05</td>\n",
       "      <td>0.002567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.203903e-04</td>\n",
       "      <td>0.013736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.010567e-03</td>\n",
       "      <td>0.049528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.495745e-03</td>\n",
       "      <td>0.164111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.985733e-03</td>\n",
       "      <td>0.407920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.100000e-04</td>\n",
       "      <td>0.794109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           left     right\n",
       "0  1.687153e-08  0.000005\n",
       "1  2.250581e-08  0.000048\n",
       "2  3.411861e-07  0.000452\n",
       "3  1.936000e-05  0.002567\n",
       "4  2.203903e-04  0.013736\n",
       "5  1.010567e-03  0.049528\n",
       "6  1.495745e-03  0.164111\n",
       "7  8.985733e-03  0.407920\n",
       "8  8.100000e-04  0.794109\n",
       "9  0.000000e+00  0.000000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q_Learning()\n",
    "Q_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
