{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    \n",
    "    # Agent will hold a map (which represent record table)\n",
    "    def __init__(self, table_size=(5,5), epsilon=0.9, alpha=0.1, gamma=0.8, action_list=['up','down','left','right']):\n",
    "        self._TABLE_SIZE = table_size\n",
    "        self._EPSILON = epsilon\n",
    "        self._ALPHA = alpha # learning rate\n",
    "        self._GAMMA = gamma\n",
    "        \n",
    "        self.ACTION_SIZE = len(action_list)\n",
    "        self.ACTION_LIST = action_list\n",
    "        \n",
    "        self.table = np.zeros(table_size + (len(action_list),), dtype='float16')  \n",
    "    \n",
    "    def new_episode(self):\n",
    "        self.state = (0,0)\n",
    "        return self.state\n",
    "    \n",
    "    def learn(self):\n",
    "        pass\n",
    "    \n",
    "class QLearningAgent(Agent):\n",
    "    def __init__(self, table_size=(5,5), epsilon=0.9, alpha=0.1, gamma=0.8, action_list=['up','down','left','right']):\n",
    "        super().__init__(table_size=table_size, epsilon=epsilon, alpha=alpha, gamma=gamma, action_list=action_list)\n",
    "    \n",
    "    def learn(self, reward, action, next_state):\n",
    "        action_idx = self.ACTION_LIST.index(action)        \n",
    "        origin = self.table[self.state+(action_idx,)]\n",
    "        #### choose max reward from next_state\n",
    "        prediction = self._GAMMA * self.table[next_state].max()\n",
    "        \n",
    "        self.table[self.state+(action_idx,)] = origin + self._ALPHA*(reward+prediction-origin)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        # epsilon-greedy\n",
    "        rate = np.random.rand()\n",
    "        if rate > self._EPSILON or (self.table[state].max()==0 and self.table[state].min()==0):\n",
    "            # choose randomly\n",
    "            action = np.random.choice(self.ACTION_LIST)\n",
    "        elif self.table[state].max()==0 and self.table[state].min!=0:\n",
    "            # choose randomly from non-negative reward action\n",
    "            p_list = np.array([0] * self.ACTION_SIZE)\n",
    "            count= 0\n",
    "            for idx, reward in enumerate(self.table[state]):\n",
    "                if reward==0:\n",
    "                    p_list[idx]=1\n",
    "                    count+=1\n",
    "            p_list = list(p_list/count)\n",
    "            actionIdx = np.random.choice(self.ACTION_SIZE, 1, p_list)[0]\n",
    "            action = self.ACTION_LIST[actionIdx]\n",
    "        else:\n",
    "            # choose the action which contain max reward\n",
    "            actionIdx_with_highestReward = self.table[state].argmax()\n",
    "            action = self.ACTION_LIST[actionIdx_with_highestReward]\n",
    "        \n",
    "        return action\n",
    "    \n",
    "class SarsaAgent(Agent):\n",
    "    def __init__(self, table_size=(5,5), epsilon=0.9, alpha=0.1, gamma=0.8, action_list=['up','down','left','right']):\n",
    "        super().__init__(table_size=table_size, epsilon=epsilon, alpha=alpha, gamma=gamma, action_list=action_list)\n",
    "    \n",
    "    def learn(self, reward, action, next_action, next_state):\n",
    "        action_idx = self.ACTION_LIST.index(action)\n",
    "        next_action_idx = self.ACTION_LIST.index(next_action)\n",
    "        \n",
    "        origin = self.table[self.state+(action_idx,)]\n",
    "        ### choose exact reward according to next state and action\n",
    "        prediction = self._GAMMA * self.table[next_state+(next_action_idx,)]\n",
    "        \n",
    "        self.table[self.state+(action_idx,)] = origin + self._ALPHA*(reward+prediction-origin)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        # epsilon-greedy\n",
    "        rate = np.random.rand()\n",
    "        if rate > self._EPSILON or (self.table[state].max()==0 and self.table[state].min()==0):\n",
    "            # choose randomly\n",
    "            action = np.random.choice(self.ACTION_LIST)\n",
    "        \n",
    "        else:\n",
    "            actionIdx_with_highestReward = self.table[state].argmax()\n",
    "            action = self.ACTION_LIST[actionIdx_with_highestReward]\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(tk.Tk):\n",
    "    def __init__(self, size=(5,5), target=(3,3), target_reward=10,\n",
    "                 fail_list=[(3,2),(2,3)], fail_punishment_list=[-5, -5],\n",
    "                 wall_punishment = -1, pxl_unit=40):\n",
    "        super().__init__()\n",
    "        self.SIZE = size\n",
    "        self.PXL_UNIT = pxl_unit\n",
    "        self.title('maze')\n",
    "        self.geometry('{0}x{1}'.format(size[0] * pxl_unit, size[1] * pxl_unit * 4))\n",
    "        \n",
    "#         self._MAP_SIZE = map_size\n",
    "        self._WALL_PUNISHMENT = wall_punishment\n",
    "        \n",
    "        self.REWARD_MAP = self._assign_reward_to_map(target, target_reward,\n",
    "                                               fail_list, fail_punishment_list)\n",
    "    \n",
    "        self._build_maze(target, fail_list)\n",
    "    \n",
    "    def _assign_reward_to_map(self, target, target_reward, fail_list, fail_punishment_list):\n",
    "        \n",
    "        tmp_map = np.zeros(self.SIZE, dtype='int')\n",
    "        \n",
    "        # assign reward when reach the target \n",
    "        tmp_map[target] = target_reward\n",
    "        \n",
    "        # assign failure punishment\n",
    "        for coordinate, punishment in zip(fail_list, fail_punishment_list):\n",
    "            tmp_map[coordinate] = punishment\n",
    "        \n",
    "        return tmp_map\n",
    "\n",
    "    def _build_maze(self, target, fail_list):\n",
    "        self.canvas = tk.Canvas(self, bg='gray',\n",
    "                           height=self.SIZE[0] * self.PXL_UNIT,\n",
    "                           width= self.SIZE[1] * self.PXL_UNIT)\n",
    "\n",
    "        # create grids\n",
    "        for c in range(0, self.SIZE[1] * self.PXL_UNIT, self.PXL_UNIT):\n",
    "            x0, y0, x1, y1 = c, 0, c, self.SIZE[1] * self.PXL_UNIT\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, self.SIZE[0] * self.PXL_UNIT, self.PXL_UNIT):\n",
    "            x0, y0, x1, y1 = 0, r, self.SIZE[0] * self.PXL_UNIT, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # create origin\n",
    "        origin = np.array([20, 20])\n",
    "        \n",
    "        # create oval\n",
    "        target_center = origin + np.array(target) * self.PXL_UNIT\n",
    "        self.oval = self.canvas.create_oval(\n",
    "            target_center[0] - 15, target_center[1] - 15,\n",
    "            target_center[0] + 15, target_center[1] + 15,\n",
    "            fill='yellow')\n",
    "        \n",
    "        for coordinate in fail_list:\n",
    "            center = origin + np.array(coordinate) * self.PXL_UNIT\n",
    "            self.canvas.create_rectangle(\n",
    "                center[1] - 15, center[0] - 15,\n",
    "                center[1] + 15, center[0] + 15,\n",
    "                fill='black')\n",
    "        \n",
    "#         # hell\n",
    "#         hell1_center = origin + np.array([self.PXL_UNIT * 2, self.PXL_UNIT])\n",
    "#         self.hell1 = self.canvas.create_rectangle(\n",
    "#             hell1_center[0] - 15, hell1_center[1] - 15,\n",
    "#             hell1_center[0] + 15, hell1_center[1] + 15,\n",
    "#             fill='black')\n",
    "#         # hell\n",
    "#         hell2_center = origin + np.array([self.PXL_UNIT, self.PXL_UNIT * 2])\n",
    "#         self.hell2 = self.canvas.create_rectangle(\n",
    "#             hell2_center[0] - 15, hell2_center[1] - 15,\n",
    "#             hell2_center[0] + 15, hell2_center[1] + 15,\n",
    "#             fill='black')\n",
    "\n",
    "        # create red rect :current state\n",
    "        self.agent_rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 10, origin[1] - 10,\n",
    "            origin[0] + 10, origin[1] + 10,\n",
    "            fill='red')\n",
    "\n",
    "        # pack all\n",
    "        self.canvas.pack()\n",
    "    \n",
    "    def take_action(self, state, action):\n",
    "        reward = 0\n",
    "        next_state = state\n",
    "        terminal = False\n",
    "        move = (0,0)\n",
    "        if action=='up':\n",
    "            if state[0]==0:\n",
    "                next_state = state # stay in place\n",
    "                reward = self._WALL_PUNISHMENT\n",
    "            else:\n",
    "                move = (-1,0)\n",
    "                next_state = (state[0]-1, state[1])\n",
    "                reward = self.REWARD_MAP[state]\n",
    "        elif action=='down':\n",
    "            if state[0]==self.SIZE[0]-1:\n",
    "                next_state = state # stay in place\n",
    "                reward = self._WALL_PUNISHMENT\n",
    "            else:\n",
    "                move=(1,0)\n",
    "                next_state = (state[0]+1, state[1])\n",
    "                reward = self.REWARD_MAP[state]\n",
    "        elif action=='left':\n",
    "            if state[1]==0:\n",
    "                next_state = state # stay in place\n",
    "                reward = self._WALL_PUNISHMENT\n",
    "            else:\n",
    "                move=(0,-1)\n",
    "                next_state = (state[0], state[1]-1)\n",
    "                reward = self.REWARD_MAP[state]\n",
    "        elif action=='right':\n",
    "            if state[1]==self.SIZE[1]-1:\n",
    "                next_state = state # stay in place\n",
    "                reward = self._WALL_PUNISHMENT\n",
    "            else:\n",
    "                move=(0,1)\n",
    "                next_state = (state[0], state[1]+1)\n",
    "                reward = self.REWARD_MAP[state]\n",
    "        \n",
    "        self.canvas.move(self.agent_rect, move[1]*self.PXL_UNIT, move[0]*self.PXL_UNIT)  # move agent\n",
    "#         self.update()\n",
    "        # check if terminal\n",
    "        if self.REWARD_MAP[next_state]!=0:\n",
    "            terminal=True\n",
    "            \n",
    "        return next_state, reward, terminal\n",
    "    \n",
    "    def reset(self):\n",
    "        self.update()\n",
    "        time.sleep(0.5)\n",
    "        self.canvas.delete(self.agent_rect)\n",
    "        origin = np.array([20, 20])\n",
    "        self.agent_rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 10, origin[1] - 10,\n",
    "            origin[0] + 10, origin[1] + 10,\n",
    "            fill='red')\n",
    "        # return observation\n",
    "#         return self.canvas.coords(self.rect)\n",
    "    def showEnvInfo(self, next_state, reward, terminal):\n",
    "        print(\"-->{}\".format(next_state),end='')\n",
    "        if terminal:\n",
    "            if self.REWARD_MAP[next_state]>0:\n",
    "                print(\"  >>>win<<<\")\n",
    "            elif self.REWARD_MAP[next_state]<0:\n",
    "                print(\"  >>>fail<<<\")\n",
    "            else:\n",
    "                print(\"(*&)(*({)(something wrong~\")\n",
    "        self.update()\n",
    "   \n",
    "    def render(self):\n",
    "#         time.sleep(0.1)\n",
    "        self.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)-->(0, 0)-->(0, 0)-->(0, 0)-->(0, 0)-->(0, 1)-->(0, 2)-->(0, 1)-->(0, 1)-->(0, 1)-->(0, 1)-->(0, 1)-->(0, 2)-->(0, 3)-->(0, 3)-->(0, 4)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bl515/anaconda/lib/python3.6/tkinter/__init__.py\", line 1699, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/Users/bl515/anaconda/lib/python3.6/tkinter/__init__.py\", line 745, in callit\n",
      "    func(*args)\n",
      "  File \"<ipython-input-15-5c2aee173092>\", line 38, in process_QLearning\n",
      "    next_state, reward, terminal = env.take_action(state, action)\n",
      "  File \"<ipython-input-14-a6785d908062>\", line 122, in take_action\n",
      "    self.canvas.move(self.agent_rect, move[1]*self.PXL_UNIT, move[0]*self.PXL_UNIT)  # move agent\n",
      "  File \"/Users/bl515/anaconda/lib/python3.6/tkinter/__init__.py\", line 2585, in move\n",
      "    self.tk.call((self._w, 'move') + args)\n",
      "_tkinter.TclError: invalid command name \".!canvas\"\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "EPISODE = 30\n",
    "STEP_DELAY = 1\n",
    "EPISODE_DELAY = 2\n",
    "######################################\n",
    "# main function\n",
    "def process_Sarsa(agent):\n",
    "    for epi in range(EPISODE):\n",
    "        state = agent.new_episode() # init state\n",
    "        action = agent.choose_action(agent.state)\n",
    "\n",
    "        print(\"episode : {}\".format(epi),end='\\n')\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            next_state, reward, terminal = env.take_action(state, action)\n",
    "            next_action = agent.choose_action(next_state)\n",
    "            ### update sarsa table\n",
    "            agent.learn(reward, action, next_action, next_state)\n",
    "            ###\n",
    "            \n",
    "            state = agent.state = next_state\n",
    "            action = next_action\n",
    "            time.sleep(STEP_DELAY)\n",
    "            \n",
    "        time.sleep(EPISODE_DELAY)\n",
    "\n",
    "def process_QLearning():\n",
    "    agent = q_agent\n",
    "    for epi in range(EPISODE):\n",
    "        env.reset()\n",
    "        state = agent.new_episode()\n",
    "        \n",
    "        print(\"{}\".format(agent.state),end='')\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            state = agent.state\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, terminal = env.take_action(state, action)\n",
    "            ### update sarsa table\n",
    "            agent.learn(reward, action, next_state)\n",
    "            ###\n",
    "            env.showEnvInfo(next_state, reward, terminal)\n",
    "            agent.state = next_state\n",
    "            time.sleep(STEP_DELAY)\n",
    "        time.sleep(EPISODE_DELAY)\n",
    "    \n",
    "################################################\n",
    "env = Env(fail_list=[(2,3),(3,1)])\n",
    "s_agent = SarsaAgent() # init agent and Q-table\n",
    "q_agent = QLearningAgent()\n",
    "\n",
    "\n",
    "env.after(1000, process_QLearning)\n",
    "env.mainloop()\n",
    "\n",
    "# process_Sarsa(s_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1,2)\n",
    "np.array(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
